# -*- coding: utf-8 -*-
"""AISC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lsEGXlD5ylH9PPl4uCV2urqnff4ZOUSE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']

df = pd.read_csv(url, names=column_names, na_values='?')

print(f"   Rows: {df.shape[0]}, Columns: {df.shape[1]}")

df.head()

df.info()

df.describe()

df.isnull().sum()

df['target'].value_counts()

df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)

print("\n7. Binary Target Distribution:")
print(f"   No Disease (0): {(df['target']==0).sum()}")
print(f"   Disease (1): {(df['target']==1).sum()}")

missing_counts = df.isnull().sum()
print(missing_counts[missing_counts > 0])

print(f"\nTotal missing values: {df.isnull().sum().sum()}")
print(f"Rows with missing values: {df.isnull().any(axis=1).sum()}")

from sklearn.impute import SimpleImputer

# Separate features and target
X_raw = df.drop('target', axis=1)
y = df['target'].values

# Impute missing values with median (better than dropping rows)
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X_raw)

df_clean = pd.DataFrame(X_imputed, columns=X_raw.columns)
df_clean['target'] = y

print(f"\n✓ Missing values imputed using median strategy")
print(f"  Original shape: {df.shape}")
print(f"  After imputation: {df_clean.shape}")
print(f"  Remaining NaN: {df_clean.isnull().sum().sum()}")

print("\n[2.2] Feature Engineering...")

# Create a copy for feature engineering
df_processed = df_clean.copy()

df_processed = df_clean.copy()

# Derived feature: Heart Rate Reserve (single calculation, with NaN-safe handling)
predicted_max_hr = 220 - df_processed['age']
df_processed['hr_reserve'] = df_processed['thalach'] - (predicted_max_hr * 0.5)
# If any NaN appear (shouldn't normally), fill with median
if df_processed['hr_reserve'].isnull().any():
    df_processed['hr_reserve'].fillna(df_processed['hr_reserve'].median(), inplace=True)

df_processed['bp_category'] = pd.cut(df_processed['trestbps'],
                                      bins=[0, 120, 130, 200],
                                      labels=['normal', 'elevated', 'high'])

df_processed['chol_category'] = pd.cut(df_processed['chol'],
                                        bins=[0, 200, 240, 600],
                                        labels=['normal', 'borderline', 'high'])

predicted_max_hr = 220 - df_processed['age']
df_processed['hr_reserve'] = df_processed['thalach'] - (predicted_max_hr * 0.5)

cp_severity = {1: 0, 2: 1, 3: 2, 4: 3}
df_processed['cp_severity'] = df_processed['cp'].map(cp_severity)

if df_processed['cp_severity'].isnull().any():
    # Fallback: fill missing with median of existing cp_severity
    df_processed['cp_severity'].fillna(int(df_processed['cp_severity'].median()), inplace=True)
df_processed['cp_severity'] = df_processed['cp_severity'].astype(int)

df_processed['age_group'] = pd.cut(df_processed['age'], bins=[20,40,60,80], labels=['young','middle','old'])

print("New derived features created:")
print("  - age_group: Categorical age ranges")
print("  - bp_category: Blood pressure risk level")
print("  - chol_category: Cholesterol risk level")
print("  - hr_reserve: Heart rate reserve capacity")
print("  - cp_severity: Chest pain severity score")

print("\n[2.3] Preparing Feature Sets...")

# Features for Neural Network (all numerical)
nn_features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
               'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal',
               'hr_reserve', 'cp_severity']

fuzzy_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

print(f"\nNeural Network will use {len(nn_features)} features:")
print(f"  {nn_features}")

print(f"\nFuzzy Logic will use {len(fuzzy_features)} key features:")
print(f"  {fuzzy_features}")

print("\n[2.4] Splitting Data into Train/Test Sets...")

# Prepare X and y
X = df_processed[nn_features].values
y = df_processed['target'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTraining set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")
print(f"\nClass distribution in training set:")
print(f"  No Disease (0): {(y_train==0).sum()} ({(y_train==0).sum()/len(y_train)*100:.1f}%)")
print(f"  Disease (1): {(y_train==1).sum()} ({(y_train==1).sum()/len(y_train)*100:.1f}%)")

print("\n[2.5] Feature Scaling...")

# Initialize scaler
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define column names for UCI Heart Disease dataset
feature_names = [
' age', 'sex', 'cp', 'trestbps', 'chol',
'fbs', 'restecg', 'thalach', 'exang',
'oldpeak', 'slope', 'ca', 'thal', 'target'
]
# If 'target' was dropped already
X_columns = feature_names[:-1]

print("Features standardized using StandardScaler (mean=0, std=1)")
print(f"\nExample - Feature 0 (age):")
print(f"  Before scaling: mean={X_train[:, 0].mean():.2f}, std={X_train[:, 0].std():.2f}")
print(f"  After scaling: mean={X_train_scaled[:, 0].mean():.2f}, std={X_train_scaled[:, 0].std():.2f}")

X_train_cnn = X_train_scaled.reshape(-1, 15, 1)
X_test_cnn = X_test_scaled.reshape(-1, 15, 1)

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(15, 1)),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

model.summary()

history = model.fit(X_train_cnn, y_train,
                    epochs=30,
                    batch_size=16,
                    validation_data=(X_test_cnn, y_test))

results = model.evaluate(X_test_cnn, y_test)
print("Test Loss, Accuracy, AUC:", results)

y_pred_proba = model.predict(X_test_cnn).flatten()

# Optional: create a DataFrame for clarity
import pandas as pd
fis_input = pd.DataFrame({
    'cnn_output': y_pred_proba,
    'chol': X_test[:, 3],      # cholesterol
    'thalach': X_test[:, 7],   # max heart rate achieved
    'true_label': y_test
})
fis_input.head()

!pip install scikit-fuzzy

import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl

# Define fuzzy variables
cnn_output = ctrl.Antecedent(np.arange(0, 1.01, 0.01), 'cnn_output')
chol = ctrl.Antecedent(np.arange(100, 400, 1), 'chol')
thalach = ctrl.Antecedent(np.arange(70, 210, 1), 'thalach')
heart_risk = ctrl.Consequent(np.arange(0, 1.01, 0.01), 'heart_risk')

cnn_output['low'] = fuzz.trimf(cnn_output.universe, [0, 0, 0.5])
cnn_output['medium'] = fuzz.trimf(cnn_output.universe, [0.25, 0.5, 0.75])
cnn_output['high'] = fuzz.trimf(cnn_output.universe, [0.5, 1, 1])

chol['low'] = fuzz.trimf(chol.universe, [100, 150, 200])
chol['medium'] = fuzz.trimf(chol.universe, [180, 220, 260])
chol['high'] = fuzz.trimf(chol.universe, [240, 300, 400])

thalach['low'] = fuzz.trimf(thalach.universe, [70, 100, 130])
thalach['medium'] = fuzz.trimf(thalach.universe, [120, 150, 180])
thalach['high'] = fuzz.trimf(thalach.universe, [160, 190, 210])

heart_risk['low'] = fuzz.trimf(heart_risk.universe, [0, 0, 0.4])
heart_risk['medium'] = fuzz.trimf(heart_risk.universe, [0.3, 0.5, 0.7])
heart_risk['high'] = fuzz.trimf(heart_risk.universe, [0.6, 1, 1])

rule1 = ctrl.Rule(cnn_output['high'] & chol['high'] & thalach['low'], heart_risk['high'])
rule2 = ctrl.Rule(cnn_output['high'] & chol['medium'] & thalach['medium'], heart_risk['high'])
rule3 = ctrl.Rule(cnn_output['medium'] & chol['high'], heart_risk['high'])
rule4 = ctrl.Rule(cnn_output['medium'] & chol['medium'] & thalach['low'], heart_risk['medium'])
rule5 = ctrl.Rule(cnn_output['low'] & chol['low'] & thalach['high'], heart_risk['low'])
rule6 = ctrl.Rule(cnn_output['low'] & chol['medium'], heart_risk['medium'])
rule7 = ctrl.Rule(cnn_output['high'] & thalach['high'], heart_risk['medium'])

heart_ctrl = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7])
heart_sim = ctrl.ControlSystemSimulation(heart_ctrl)

sample_idx = 0
sample_pred = model.predict(X_test_cnn[sample_idx].reshape(1, 15, 1))[0][0]

# Use column indices based on known UCI order
chol_index = X_columns.index('chol')
thalach_index = X_columns.index('thalach')

chol_val = X_test [sample_idx, chol_index]
thalach_val = X_test [sample_idx, thalach_index]

heart_sim.input['cnn_output'] = sample_pred
heart_sim.input['chol'] = chol_val
heart_sim.input['thalach'] = thalach_val

heart_sim.compute()

print(f"CNN predicted probability: {sample_pred:.3f}")
print(f"Cholesterol: {chol_val:.1f}, Max HR (thalach): {thalach_val:.1f}")
print(f"Fuzzy heart risk (defuzzified output): {heart_sim.output['heart_risk']:.3f}")

heart_risk.view(sim=heart_sim)

fuzzy_outputs = []
for i in range(len(X_test)):
    sample_pred = model.predict(X_test_cnn[i].reshape(1, 15, 1))[0][0]

    chol_val = X_test[i, chol_index]
    thalach_val = X_test[i, thalach_index]

    heart_sim.input['cnn_output'] = sample_pred
    heart_sim.input['chol'] = chol_val
    heart_sim.input['thalach'] = thalach_val

    try:
        heart_sim.compute()
        fuzzy_outputs.append(heart_sim.output['heart_risk'])
    except KeyError:
        fuzzy_outputs.append(sample_pred)
fuzzy_outputs = np.array(fuzzy_outputs)

fuzzy_preds = (fuzzy_outputs >= 0.5).astype(int)

from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix

acc = accuracy_score(y_test, fuzzy_preds)
auc = roc_auc_score(y_test, fuzzy_outputs)
cm = confusion_matrix(y_test, fuzzy_preds)

print("Hybrid CNN–FIS Model Evaluation:")
print(f"Accuracy: {acc:.3f}")
print(f"AUC: {auc:.3f}")
print("Confusion Matrix:\n", cm)

import matplotlib.pyplot as plt

# Compare CNN and Fuzzy outputs
cnn_outputs = model.predict(X_test_cnn).flatten()

plt.figure(figsize=(7,5))
plt.scatter(cnn_outputs, fuzzy_outputs, c=y_test, cmap='coolwarm', edgecolors='k')
plt.xlabel("CNN Predicted Probability")
plt.ylabel("Fuzzy Heart Risk Output")
plt.title("CNN vs Fuzzy-Adjusted Predictions")
plt.colorbar(label="True Class (0=Healthy, 1=Disease)")
plt.grid(True)
plt.show()

sample_indices = [0, 5, 10, 15, 20]  # you can adjust these later

rows = []
for i in sample_indices:
    cnn_pred = model.predict(X_test_cnn[i].reshape(1, 15, 1))[0][0]
    chol_val = X_test[i, chol_index]
    thalach_val = X_test[i, thalach_index]

    heart_sim.input['cnn_output'] = cnn_pred
    heart_sim.input['chol'] = chol_val
    heart_sim.input['thalach'] = thalach_val
    heart_sim.compute()
    fuzzy_val = heart_sim.output['heart_risk']

    rows.append({
        "Index": i,
        "True Label": int(y_test[i]),
        "CNN Prob": round(float(cnn_pred), 3),
        "Cholesterol": round(float(chol_val), 1),
        "Thalach": round(float(thalach_val), 1),
        "Fuzzy Heart Risk": round(float(fuzzy_val), 3),
    })

explain_table = pd.DataFrame(rows)
print("\nExplainability Table — Sample Predictions:\n")
print(explain_table.to_string(index=False))

def fuzzy_reasoning_explanation(cnn_prob, chol_val, thalach_val, fuzzy_val):
    """
    Generate a natural-language explanation of the hybrid model's reasoning.
    """

    # Linguistic interpretation of CNN output
    if cnn_prob < 0.4:
        cnn_level = "low"
    elif cnn_prob < 0.7:
        cnn_level = "moderate"
    else:
        cnn_level = "high"

    # Linguistic interpretation of cholesterol
    if chol_val < 200:
        chol_level = "low"
    elif chol_val < 240:
        chol_level = "moderate"
    else:
        chol_level = "high"

    # Linguistic interpretation of thalach (max heart rate)
    if thalach_val < 130:
        thalach_level = "low"
    elif thalach_val < 170:
        thalach_level = "moderate"
    else:
        thalach_level = "high"

    # Linguistic interpretation of final fuzzy risk
    if fuzzy_val < 0.4:
        risk_level = "low"
    elif fuzzy_val < 0.7:
        risk_level = "moderate"
    else:
        risk_level = "high"

    explanation = (
        f"The CNN predicted a {cnn_level} probability of heart disease ({cnn_prob:.2f}). "
        f"The patient's cholesterol is {chol_level} ({chol_val:.0f} mg/dL) and "
        f"maximum heart rate (thalach) is {thalach_level} ({thalach_val:.0f} bpm). "
        f"Based on these factors, the fuzzy reasoning system inferred a {risk_level} overall heart risk ({fuzzy_val:.2f})."
    )

    return explanation

print("\nSample Explanations:\n")

for i in [0, 5, 10]:
    cnn_pred = model.predict(X_test_cnn[i].reshape(1, 15, 1))[0][0]
    chol_val = X_test[i, chol_index]
    thalach_val = X_test[i, thalach_index]

    heart_sim.input['cnn_output'] = cnn_pred
    heart_sim.input['chol'] = chol_val
    heart_sim.input['thalach'] = thalach_val
    heart_sim.compute()
    fuzzy_val = heart_sim.output['heart_risk']

    print(f"Patient {i} → True Label: {y_test[i]}")
    print(fuzzy_reasoning_explanation(cnn_pred, chol_val, thalach_val, fuzzy_val))
    print("-" * 80)